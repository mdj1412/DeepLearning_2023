{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Tu9axwWtA5teIvefJkoHUIXMZiC2f2eX","timestamp":1679640642294},{"file_id":"19qiFpL5C6QBV0_M16GEBxFw3ttjTLVL-","timestamp":1652407061152}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# MLP로 Iris 분류하기\n","by uramoon@kw.ac.kr\n","(<a href=\"https://raw.githubusercontent.com/ronreiter/interactive-tutorials/master/LICENSE\">Apache 2.0 License</a>)"],"metadata":{"id":"ruiMWoyy72qJ"}},{"cell_type":"markdown","source":["꽃받침 길이 (cm), 꽃받침 너비 (cm), 꽃잎 길이 (cm), 꽃잎 너비 (cm)로 Iris setosa, Iris versicolor, Iris virginica를 분류하는 MLP를 만들어 봅시다."],"metadata":{"id":"Xh_usCahkFnI"}},{"cell_type":"markdown","source":["## 데이터셋 만들기\n","seaborn에서 제공하는 Iris 데이터셋을 사용합니다.<br>\n","seaborn은 데이터 시각화 도구입니다. (https://seaborn.pydata.org/)"],"metadata":{"id":"L8HUu50d77w2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_N1eAZQd7ymK","executionInfo":{"status":"ok","timestamp":1679668302517,"user_tz":-540,"elapsed":2431,"user":{"displayName":"민동준","userId":"06420174620131971170"}}},"outputs":[],"source":["# seaborn 라이브러리로 iris 데이터셋 DataFrame으로 읽어오기\n","import seaborn as sns\n","iris = sns.load_dataset(\"iris\")"]},{"cell_type":"markdown","source":["### 데이터셋 살펴보기\n","\n","[*pandas*](http://pandas.pydata.org/)를 이용해 데이터를 처리합니다. pandas는 데이터 처리 및 분석에 사용되는 오픈소스 라이브러리입니다. <br>\n","iris는 pandas DataFrame으로 저장되어 있습니다."],"metadata":{"id":"Ay6_zzP99Yv3"}},{"cell_type":"code","source":["# TODO: 무작위로 10개의 샘플을 확인해보세요.\n","\n","print(type(iris), iris.shape)\n","iris.sample(10)"],"metadata":{"id":"XB3W9ITx9g-A","colab":{"base_uri":"https://localhost:8080/","height":381},"executionInfo":{"status":"ok","timestamp":1679668302517,"user_tz":-540,"elapsed":8,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"abe923c4-4c60-4ab1-eef7-a6e0eaa3fbc3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'> (150, 5)\n"]},{"output_type":"execute_result","data":{"text/plain":["     sepal_length  sepal_width  petal_length  petal_width     species\n","59            5.2          2.7           3.9          1.4  versicolor\n","50            7.0          3.2           4.7          1.4  versicolor\n","30            4.8          3.1           1.6          0.2      setosa\n","23            5.1          3.3           1.7          0.5      setosa\n","1             4.9          3.0           1.4          0.2      setosa\n","126           6.2          2.8           4.8          1.8   virginica\n","58            6.6          2.9           4.6          1.3  versicolor\n","84            5.4          3.0           4.5          1.5  versicolor\n","26            5.0          3.4           1.6          0.4      setosa\n","7             5.0          3.4           1.5          0.2      setosa"],"text/html":["\n","  <div id=\"df-91b9e4fc-e7f4-4487-9643-628b32a91785\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>59</th>\n","      <td>5.2</td>\n","      <td>2.7</td>\n","      <td>3.9</td>\n","      <td>1.4</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>7.0</td>\n","      <td>3.2</td>\n","      <td>4.7</td>\n","      <td>1.4</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>4.8</td>\n","      <td>3.1</td>\n","      <td>1.6</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>5.1</td>\n","      <td>3.3</td>\n","      <td>1.7</td>\n","      <td>0.5</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.9</td>\n","      <td>3.0</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>126</th>\n","      <td>6.2</td>\n","      <td>2.8</td>\n","      <td>4.8</td>\n","      <td>1.8</td>\n","      <td>virginica</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>6.6</td>\n","      <td>2.9</td>\n","      <td>4.6</td>\n","      <td>1.3</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>5.4</td>\n","      <td>3.0</td>\n","      <td>4.5</td>\n","      <td>1.5</td>\n","      <td>versicolor</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>5.0</td>\n","      <td>3.4</td>\n","      <td>1.6</td>\n","      <td>0.4</td>\n","      <td>setosa</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>5.0</td>\n","      <td>3.4</td>\n","      <td>1.5</td>\n","      <td>0.2</td>\n","      <td>setosa</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91b9e4fc-e7f4-4487-9643-628b32a91785')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-91b9e4fc-e7f4-4487-9643-628b32a91785 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-91b9e4fc-e7f4-4487-9643-628b32a91785');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["데이터의 통계자료를 확인해봅시다."],"metadata":{"id":"OF5RtzaJyw_F"}},{"cell_type":"code","source":["# TODO: 통계자료를 확인해보세요. (일부 컬럼은 누락될 수 있습니다.)\n","# Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n","\n","iris.describe()"],"metadata":{"id":"Ehqxz6i0yzZA","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1679668302518,"user_tz":-540,"elapsed":5,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"6d6b3853-33d2-421f-da7f-46ec291d0043"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       sepal_length  sepal_width  petal_length  petal_width\n","count    150.000000   150.000000    150.000000   150.000000\n","mean       5.843333     3.057333      3.758000     1.199333\n","std        0.828066     0.435866      1.765298     0.762238\n","min        4.300000     2.000000      1.000000     0.100000\n","25%        5.100000     2.800000      1.600000     0.300000\n","50%        5.800000     3.000000      4.350000     1.300000\n","75%        6.400000     3.300000      5.100000     1.800000\n","max        7.900000     4.400000      6.900000     2.500000"],"text/html":["\n","  <div id=\"df-7ebf0b7a-5d8d-4c44-9f1d-dd9508838e3b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal_length</th>\n","      <th>sepal_width</th>\n","      <th>petal_length</th>\n","      <th>petal_width</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>150.000000</td>\n","      <td>150.000000</td>\n","      <td>150.000000</td>\n","      <td>150.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>5.843333</td>\n","      <td>3.057333</td>\n","      <td>3.758000</td>\n","      <td>1.199333</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.828066</td>\n","      <td>0.435866</td>\n","      <td>1.765298</td>\n","      <td>0.762238</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>4.300000</td>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>0.100000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>5.100000</td>\n","      <td>2.800000</td>\n","      <td>1.600000</td>\n","      <td>0.300000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>5.800000</td>\n","      <td>3.000000</td>\n","      <td>4.350000</td>\n","      <td>1.300000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>6.400000</td>\n","      <td>3.300000</td>\n","      <td>5.100000</td>\n","      <td>1.800000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>7.900000</td>\n","      <td>4.400000</td>\n","      <td>6.900000</td>\n","      <td>2.500000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ebf0b7a-5d8d-4c44-9f1d-dd9508838e3b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7ebf0b7a-5d8d-4c44-9f1d-dd9508838e3b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7ebf0b7a-5d8d-4c44-9f1d-dd9508838e3b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# 히스토그램 그리기\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","iris.hist(bins=50, figsize=(10,5))\n","plt.show()"],"metadata":{"id":"7M6ycB-ZzXMP","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1679668302976,"user_tz":-540,"elapsed":462,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"e40ac70d-9440-4e5a-8f0d-0df14ed2e4bc"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 4 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["# TODO: 각 컬럼의 데이터 타입과 null 값을 지닌 컬럼이 있는지 확인해보세요.\n","# Hint: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html\n","\n","iris.info()"],"metadata":{"id":"EHysgSA59y7d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668302976,"user_tz":-540,"elapsed":9,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"3e0ae0ae-68b6-4bc2-e7b5-dda853a271ac"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150 entries, 0 to 149\n","Data columns (total 5 columns):\n"," #   Column        Non-Null Count  Dtype  \n","---  ------        --------------  -----  \n"," 0   sepal_length  150 non-null    float64\n"," 1   sepal_width   150 non-null    float64\n"," 2   petal_length  150 non-null    float64\n"," 3   petal_width   150 non-null    float64\n"," 4   species       150 non-null    object \n","dtypes: float64(4), object(1)\n","memory usage: 6.0+ KB\n"]}]},{"cell_type":"markdown","source":["꽃받침(sepal)의 길이, 너비와 꽃잎(petal)의 길이, 너비로 어떤 꽃인지 예측할 것입니다."],"metadata":{"id":"WS0LwD9x-Xk9"}},{"cell_type":"code","source":["# TODO: 어떤 꽃들이 있는지 알아봅시다.\n","# Hint: https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\n","iris['species'].value_counts()"],"metadata":{"id":"R9Hs710E-Udc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668302976,"user_tz":-540,"elapsed":7,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"eb6a87b6-dee0-4635-d455-dac941d020fb"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["setosa        50\n","versicolor    50\n","virginica     50\n","Name: species, dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### 훈련 데이터, 검증 데이터, 테스트 데이터로 나누기\n","1. 훈련 데이터: 모델을 훈련할 때 직접적으로 사용\n","2. 검증 데이터: 모델을 훈련할 때 간접적으로 사용\n","3. 테스트 데이터: 훈련이 끝난 모델의 성능을 평가할 때 사용\n","\n","일반적으로 훈련 (60%), 검증 (20%), 테스트 (20%)로 나눕니다. <br>\n","훈련 (80%), 테스트 (20%)로 나눈 후 훈련을 다시 훈련 (80%)과 검증 (20%)으로 나누기도 합니다.<br>\n","\n","모두 같은 결과를 얻기 위해 train_test_split 함수에 random_state 42를 사용합니다.<br>\n","42는 '은하수를 여행하는 히치하이커를 위한 안내서'에 등장한 숫자인데 random seed로 종종 사용됩니다."],"metadata":{"id":"GYT86htM_Ci-"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# 훈련 (80%), 테스트 (20%)\n","train, test = train_test_split(iris, test_size=0.2, random_state=42)"],"metadata":{"id":"APq_WJ27AhDn","executionInfo":{"status":"ok","timestamp":1679668303546,"user_tz":-540,"elapsed":575,"user":{"displayName":"민동준","userId":"06420174620131971170"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# TODO: train과 test 샘플 수를 확인하세요.\n","print(len(train))\n","print(len(test))\n","\n","print(train)\n","print(test)"],"metadata":{"id":"FAcswyuuBdW1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303547,"user_tz":-540,"elapsed":25,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"780e7f06-a280-4206-b113-1f71e062e094"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["120\n","30\n","     sepal_length  sepal_width  petal_length  petal_width     species\n","22            4.6          3.6           1.0          0.2      setosa\n","15            5.7          4.4           1.5          0.4      setosa\n","65            6.7          3.1           4.4          1.4  versicolor\n","11            4.8          3.4           1.6          0.2      setosa\n","42            4.4          3.2           1.3          0.2      setosa\n","..            ...          ...           ...          ...         ...\n","71            6.1          2.8           4.0          1.3  versicolor\n","106           4.9          2.5           4.5          1.7   virginica\n","14            5.8          4.0           1.2          0.2      setosa\n","92            5.8          2.6           4.0          1.2  versicolor\n","102           7.1          3.0           5.9          2.1   virginica\n","\n","[120 rows x 5 columns]\n","     sepal_length  sepal_width  petal_length  petal_width     species\n","73            6.1          2.8           4.7          1.2  versicolor\n","18            5.7          3.8           1.7          0.3      setosa\n","118           7.7          2.6           6.9          2.3   virginica\n","78            6.0          2.9           4.5          1.5  versicolor\n","76            6.8          2.8           4.8          1.4  versicolor\n","31            5.4          3.4           1.5          0.4      setosa\n","64            5.6          2.9           3.6          1.3  versicolor\n","141           6.9          3.1           5.1          2.3   virginica\n","68            6.2          2.2           4.5          1.5  versicolor\n","82            5.8          2.7           3.9          1.2  versicolor\n","110           6.5          3.2           5.1          2.0   virginica\n","12            4.8          3.0           1.4          0.1      setosa\n","36            5.5          3.5           1.3          0.2      setosa\n","9             4.9          3.1           1.5          0.1      setosa\n","19            5.1          3.8           1.5          0.3      setosa\n","56            6.3          3.3           4.7          1.6  versicolor\n","104           6.5          3.0           5.8          2.2   virginica\n","69            5.6          2.5           3.9          1.1  versicolor\n","55            5.7          2.8           4.5          1.3  versicolor\n","132           6.4          2.8           5.6          2.2   virginica\n","29            4.7          3.2           1.6          0.2      setosa\n","127           6.1          3.0           4.9          1.8   virginica\n","26            5.0          3.4           1.6          0.4      setosa\n","128           6.4          2.8           5.6          2.1   virginica\n","131           7.9          3.8           6.4          2.0   virginica\n","145           6.7          3.0           5.2          2.3   virginica\n","108           6.7          2.5           5.8          1.8   virginica\n","143           6.8          3.2           5.9          2.3   virginica\n","45            4.8          3.0           1.4          0.3      setosa\n","30            4.8          3.1           1.6          0.2      setosa\n"]}]},{"cell_type":"markdown","source":["## X값과 y값 나누기\n","지도 학습에서는 데이터 (X)와 답 (y)을 분리하여 모델에 넣어줍니다. X는 여러 값을 지닌 벡터라서 대문자로 표기하고, y는 하나의 값을 지녀 소문자로 표기합니다. (예: MNIST의 X는 784개의 픽셀 값, y는 [0, 9]에 속한 정수)"],"metadata":{"id":"3WUl5G_ODBDk"}},{"cell_type":"code","source":["train_X = train.drop(['species'], axis=1)   # species 열(axis=1)만 제거\n","train_y = train['species']                  # species 열만 가져오기\n","\n","# TODO\n","test_X = test.drop(['species'], axis=1)\n","test_y = test['species']\n","\n","\n","print(train_X.shape, test_X.shape)\n","print(train_y.shape, test_y.shape)"],"metadata":{"id":"q9mwi5U2DL9Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303547,"user_tz":-540,"elapsed":21,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"1ce4ddd0-eb5d-441b-ce56-72bd86fb1473"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 4) (30, 4)\n","(120,) (30,)\n"]}]},{"cell_type":"markdown","source":["### y값 숫자로 변환하기\n","\n","인공 신경망에는 setosa, versicolor 등의 string이 입력될 수 없으므로 숫자로 변환해야 합니다."],"metadata":{"id":"pis2K3rnI8YX"}},{"cell_type":"code","source":["# 현재의 y값 확인\n","print(train_y)\n","# print(train_y.values)\n","\n","import numpy as np\n","np.unique(train_y.values)"],"metadata":{"id":"MELPFUREIuA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303548,"user_tz":-540,"elapsed":19,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"618136d0-f74b-4797-dac4-8351d135fed5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["22         setosa\n","15         setosa\n","65     versicolor\n","11         setosa\n","42         setosa\n","          ...    \n","71     versicolor\n","106     virginica\n","14         setosa\n","92     versicolor\n","102     virginica\n","Name: species, Length: 120, dtype: object\n"]},{"output_type":"execute_result","data":{"text/plain":["array(['setosa', 'versicolor', 'virginica'], dtype=object)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# LabelEncoder를 사용하여 string 형태의 레이블을 자동으로 변환하도록 설정\n","from sklearn import preprocessing\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(train_y)"],"metadata":{"id":"hEnx4YQKJKTK","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1679668303549,"user_tz":-540,"elapsed":18,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"9f564dc3-ac6b-41ff-8516-eddb1a0acd66"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LabelEncoder()"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 다음의 클래스들이 0, 1, 2로 맵핑됩니다.\n","le.classes_"],"metadata":{"id":"kJL3HVeyJUDi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303549,"user_tz":-540,"elapsed":16,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"8afc3779-8e68-4a48-e409-0d75939dcb21"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['setosa', 'versicolor', 'virginica'], dtype=object)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# 훈련 레이블 숫자로 변환하기\n","train_y = le.transform(train_y)\n","train_y"],"metadata":{"id":"2QxW6zwwJeFW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303549,"user_tz":-540,"elapsed":15,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"d072339b-14e0-451b-c5c8-4e93b04d677b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2, 1, 2, 1, 2,\n","       1, 0, 2, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 0, 2, 2,\n","       1, 1, 2, 1, 0, 1, 2, 0, 0, 1, 1, 0, 2, 0, 0, 1, 1, 2, 1, 2, 2, 1,\n","       0, 0, 2, 2, 0, 0, 0, 1, 2, 0, 2, 2, 0, 1, 1, 2, 1, 2, 0, 2, 1, 2,\n","       1, 1, 1, 0, 1, 1, 0, 1, 2, 2, 0, 1, 2, 2, 0, 2, 0, 1, 2, 2, 1, 2,\n","       1, 1, 2, 2, 0, 1, 2, 0, 1, 2])"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# TODO: 테스트 레이블 숫자로 변환하기\n","test_y = le.transform(test_y)\n","test_y\n","\n","print(train_X.shape, test_X.shape)\n","print(train_y.shape, test_y.shape)"],"metadata":{"id":"qM8k31yKJwMd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303549,"user_tz":-540,"elapsed":14,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"8eb60bbc-9347-44cf-a304-ac476f49f92c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["(120, 4) (30, 4)\n","(120,) (30,)\n"]}]},{"cell_type":"markdown","source":["## 전처리 하기\n","Min-max normalization으로 모든 인자의 값을 0과 1사이의 값으로 변환합니다. <br>(https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization))<br>\n","**(중요) 모든 전처리 기법은 훈련 데이터만 사용하여 학습한 후 다른 데이터 (검증 및 테스트)에 동일하게 적용합니다.**"],"metadata":{"id":"-dZbQMBpB4xc"}},{"cell_type":"code","source":["# 현재 훈련 데이터에서 가장 작은 값과 큰 값을 확인합니다.\n","import numpy as np\n","\n","print(train_X.min()) \n","print(train_X.max())"],"metadata":{"id":"O7SmI4DVsyew","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303550,"user_tz":-540,"elapsed":13,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"c447c901-abd0-4482-8955-f65ab5f90b12"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["sepal_length    4.3\n","sepal_width     2.0\n","petal_length    1.0\n","petal_width     0.1\n","dtype: float64\n","sepal_length    7.7\n","sepal_width     4.4\n","petal_length    6.7\n","petal_width     2.5\n","dtype: float64\n"]}]},{"cell_type":"code","source":["# train_X의 min, max값으로 scaler를 설정합니다.\n","from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaler.fit(train_X)"],"metadata":{"id":"FcJXMG4ACb2V","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1679668303550,"user_tz":-540,"elapsed":12,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"fbede453-ab2b-43de-e44e-394a40261be1"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MinMaxScaler()"],"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# 0부터 1사이의 NumPy 배열로 변환\n","train_X = scaler.transform(train_X)\n","\n","# TODO\n","test_X = scaler.transform(test_X)"],"metadata":{"id":"u1ZsUbnpD9yF","executionInfo":{"status":"ok","timestamp":1679668303550,"user_tz":-540,"elapsed":12,"user":{"displayName":"민동준","userId":"06420174620131971170"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 잘 변환됐는지 확인\n","\n","print(train_X.min()) \n","print(train_X.max())\n","\n","# print(train_X)\n","print(train_X.shape, test_X.shape)\n","print(train_y.shape, test_y.shape)"],"metadata":{"id":"aIvaPHMGHFzA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668303550,"user_tz":-540,"elapsed":11,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"416d56b7-8160-4071-aad0-d15dd50ec13d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["0.0\n","1.0\n","(120, 4) (30, 4)\n","(120,) (30,)\n"]}]},{"cell_type":"markdown","source":["## 모델 만들기\n","MLP에서 숨겨진 층의 노드 개수는 입력층의 2/3 정도로 설정하는 것이 일반적이지만 최적이 아닐 수도 있으니 직접 실험을 해가며 설정하는 것이 좋습니다. 출력층의 활성화 함수는 출력 중 큰 값에 높은 확률을 할당해주는 함수를 사용합니다. (https://en.wikipedia.org/wiki/Softmax_function)"],"metadata":{"id":"Apcy5bXLH0Rb"}},{"cell_type":"code","source":["from keras import models\n","from keras import layers\n","from keras.callbacks import EarlyStopping\n","\n","model = models.Sequential()\n","model.add(layers.Input(shape=(4, ))) # 입력층의 모양\n","model.add(layers.Dense(100, activation='relu')) # 숨겨진 층, 원하는 경우 층을 추가하셔도 좋습니다.\n","model.add(layers.Dense(3, activation='softmax')) # 출력층의 모양 및 활성화 함수"],"metadata":{"id":"kqMLovSRH13P","executionInfo":{"status":"ok","timestamp":1679668312363,"user_tz":-540,"elapsed":8823,"user":{"displayName":"민동준","userId":"06420174620131971170"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# TODO: 컴파일하기\n","# 레이블에 one-hot 인코딩을 사용하지 않았으므로 loss에 주의합니다.\n","\n","model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"4lJBMxwhKhWN","executionInfo":{"status":"ok","timestamp":1679668312363,"user_tz":-540,"elapsed":10,"user":{"displayName":"민동준","userId":"06420174620131971170"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["## 모델 훈련하기\n","훈련 데이터를 사용하여 인공신경망의 가중치를 조절합니다. 한 epoch이 끝나면 (훈련 데이터를 완전히 한 번 보고나면) 검증 데이터의 loss를 측정합니다. 훈련 데이터의 loss는 줄어드는데 검증 데이터의 loss가 증가하면 과적합이 일어나는 신호이므로 훈련을 중단합니다. 일시적인 현상에 콜백 함수가 속을 수 있으니 val_loss가 감소하지 않아도 10번의 epochs을 참을 수 있도록 설정해봅시다.\n"],"metadata":{"id":"W1KMQUIOK5Qv"}},{"cell_type":"code","source":["# TODO: 훈련 데이터와 정답으로 모델을 훈련합니다.\n","\n","model.fit(train_X, train_y, epochs=10000, \n","            callbacks=EarlyStopping(patience=10),            # val_loss를 관찰하여 과적합이 일어나면 훈련을 중단합니다. (10번은 참기)\n","            validation_split=0.2)       # 훈련 데이터의 20%를 검증 데이터로 사용합니다."],"metadata":{"id":"FtDX6bh5K3Cq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668339031,"user_tz":-540,"elapsed":26677,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"4f414298-5277-4e68-f7c1-17f5d0d7bc1c"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10000\n","3/3 [==============================] - 8s 194ms/step - loss: 1.0854 - accuracy: 0.5312 - val_loss: 1.1112 - val_accuracy: 0.4167\n","Epoch 2/10000\n","3/3 [==============================] - 0s 40ms/step - loss: 1.0505 - accuracy: 0.6146 - val_loss: 1.0773 - val_accuracy: 0.5000\n","Epoch 3/10000\n","3/3 [==============================] - 0s 60ms/step - loss: 1.0260 - accuracy: 0.6771 - val_loss: 1.0534 - val_accuracy: 0.5000\n","Epoch 4/10000\n","3/3 [==============================] - 0s 56ms/step - loss: 1.0066 - accuracy: 0.7188 - val_loss: 1.0322 - val_accuracy: 0.5000\n","Epoch 5/10000\n","3/3 [==============================] - 0s 44ms/step - loss: 0.9858 - accuracy: 0.7188 - val_loss: 1.0130 - val_accuracy: 0.5000\n","Epoch 6/10000\n","3/3 [==============================] - 0s 51ms/step - loss: 0.9680 - accuracy: 0.7188 - val_loss: 0.9940 - val_accuracy: 0.5000\n","Epoch 7/10000\n","3/3 [==============================] - 0s 49ms/step - loss: 0.9511 - accuracy: 0.7188 - val_loss: 0.9770 - val_accuracy: 0.5000\n","Epoch 8/10000\n","3/3 [==============================] - 0s 56ms/step - loss: 0.9331 - accuracy: 0.7188 - val_loss: 0.9586 - val_accuracy: 0.5000\n","Epoch 9/10000\n","3/3 [==============================] - 0s 65ms/step - loss: 0.9171 - accuracy: 0.7500 - val_loss: 0.9429 - val_accuracy: 0.5000\n","Epoch 10/10000\n","3/3 [==============================] - 0s 54ms/step - loss: 0.8998 - accuracy: 0.7188 - val_loss: 0.9240 - val_accuracy: 0.5000\n","Epoch 11/10000\n","3/3 [==============================] - 0s 52ms/step - loss: 0.8841 - accuracy: 0.7292 - val_loss: 0.9076 - val_accuracy: 0.5000\n","Epoch 12/10000\n","3/3 [==============================] - 0s 44ms/step - loss: 0.8680 - accuracy: 0.7292 - val_loss: 0.8909 - val_accuracy: 0.5000\n","Epoch 13/10000\n","3/3 [==============================] - 0s 46ms/step - loss: 0.8520 - accuracy: 0.7604 - val_loss: 0.8769 - val_accuracy: 0.5417\n","Epoch 14/10000\n","3/3 [==============================] - 0s 45ms/step - loss: 0.8365 - accuracy: 0.7708 - val_loss: 0.8599 - val_accuracy: 0.7500\n","Epoch 15/10000\n","3/3 [==============================] - 0s 44ms/step - loss: 0.8214 - accuracy: 0.8125 - val_loss: 0.8466 - val_accuracy: 0.7500\n","Epoch 16/10000\n","3/3 [==============================] - 0s 48ms/step - loss: 0.8060 - accuracy: 0.8021 - val_loss: 0.8296 - val_accuracy: 0.7500\n","Epoch 17/10000\n","3/3 [==============================] - 0s 58ms/step - loss: 0.7922 - accuracy: 0.8438 - val_loss: 0.8177 - val_accuracy: 0.7500\n","Epoch 18/10000\n","3/3 [==============================] - 0s 45ms/step - loss: 0.7764 - accuracy: 0.8438 - val_loss: 0.8023 - val_accuracy: 0.7917\n","Epoch 19/10000\n","3/3 [==============================] - 0s 44ms/step - loss: 0.7620 - accuracy: 0.8542 - val_loss: 0.7895 - val_accuracy: 0.7917\n","Epoch 20/10000\n","3/3 [==============================] - 0s 61ms/step - loss: 0.7495 - accuracy: 0.8229 - val_loss: 0.7750 - val_accuracy: 0.8333\n","Epoch 21/10000\n","3/3 [==============================] - 0s 57ms/step - loss: 0.7347 - accuracy: 0.8646 - val_loss: 0.7623 - val_accuracy: 0.8333\n","Epoch 22/10000\n","3/3 [==============================] - 0s 52ms/step - loss: 0.7210 - accuracy: 0.8750 - val_loss: 0.7489 - val_accuracy: 0.8333\n","Epoch 23/10000\n","3/3 [==============================] - 0s 80ms/step - loss: 0.7075 - accuracy: 0.8646 - val_loss: 0.7361 - val_accuracy: 0.8333\n","Epoch 24/10000\n","3/3 [==============================] - 0s 48ms/step - loss: 0.6943 - accuracy: 0.8750 - val_loss: 0.7236 - val_accuracy: 0.8750\n","Epoch 25/10000\n","3/3 [==============================] - 0s 33ms/step - loss: 0.6811 - accuracy: 0.8750 - val_loss: 0.7124 - val_accuracy: 0.8750\n","Epoch 26/10000\n","3/3 [==============================] - 0s 38ms/step - loss: 0.6685 - accuracy: 0.8958 - val_loss: 0.6990 - val_accuracy: 0.9167\n","Epoch 27/10000\n","3/3 [==============================] - 0s 52ms/step - loss: 0.6566 - accuracy: 0.8750 - val_loss: 0.6879 - val_accuracy: 0.9167\n","Epoch 28/10000\n","3/3 [==============================] - 0s 45ms/step - loss: 0.6439 - accuracy: 0.8542 - val_loss: 0.6793 - val_accuracy: 0.9167\n","Epoch 29/10000\n","3/3 [==============================] - 0s 39ms/step - loss: 0.6352 - accuracy: 0.8542 - val_loss: 0.6671 - val_accuracy: 0.9167\n","Epoch 30/10000\n","3/3 [==============================] - 0s 32ms/step - loss: 0.6226 - accuracy: 0.8333 - val_loss: 0.6593 - val_accuracy: 0.9167\n","Epoch 31/10000\n","3/3 [==============================] - 0s 54ms/step - loss: 0.6114 - accuracy: 0.8854 - val_loss: 0.6520 - val_accuracy: 0.9167\n","Epoch 32/10000\n","3/3 [==============================] - 0s 59ms/step - loss: 0.6007 - accuracy: 0.8854 - val_loss: 0.6421 - val_accuracy: 0.9167\n","Epoch 33/10000\n","3/3 [==============================] - 0s 41ms/step - loss: 0.5905 - accuracy: 0.8750 - val_loss: 0.6325 - val_accuracy: 0.9167\n","Epoch 34/10000\n","3/3 [==============================] - 0s 38ms/step - loss: 0.5830 - accuracy: 0.8542 - val_loss: 0.6246 - val_accuracy: 0.9167\n","Epoch 35/10000\n","3/3 [==============================] - 0s 33ms/step - loss: 0.5717 - accuracy: 0.8646 - val_loss: 0.6161 - val_accuracy: 0.9167\n","Epoch 36/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.5629 - accuracy: 0.9167 - val_loss: 0.6073 - val_accuracy: 0.9167\n","Epoch 37/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.5545 - accuracy: 0.8750 - val_loss: 0.5993 - val_accuracy: 0.9167\n","Epoch 38/10000\n","3/3 [==============================] - 0s 41ms/step - loss: 0.5465 - accuracy: 0.8854 - val_loss: 0.5931 - val_accuracy: 0.9167\n","Epoch 39/10000\n","3/3 [==============================] - 0s 38ms/step - loss: 0.5385 - accuracy: 0.9167 - val_loss: 0.5846 - val_accuracy: 0.9167\n","Epoch 40/10000\n","3/3 [==============================] - 0s 51ms/step - loss: 0.5305 - accuracy: 0.8750 - val_loss: 0.5809 - val_accuracy: 0.9167\n","Epoch 41/10000\n","3/3 [==============================] - 0s 70ms/step - loss: 0.5237 - accuracy: 0.8646 - val_loss: 0.5757 - val_accuracy: 0.9167\n","Epoch 42/10000\n","3/3 [==============================] - 0s 58ms/step - loss: 0.5148 - accuracy: 0.8750 - val_loss: 0.5707 - val_accuracy: 0.9167\n","Epoch 43/10000\n","3/3 [==============================] - 0s 63ms/step - loss: 0.5099 - accuracy: 0.8958 - val_loss: 0.5628 - val_accuracy: 0.9167\n","Epoch 44/10000\n","3/3 [==============================] - 0s 95ms/step - loss: 0.5037 - accuracy: 0.8646 - val_loss: 0.5601 - val_accuracy: 0.9167\n","Epoch 45/10000\n","3/3 [==============================] - 0s 85ms/step - loss: 0.4957 - accuracy: 0.8958 - val_loss: 0.5544 - val_accuracy: 0.9167\n","Epoch 46/10000\n","3/3 [==============================] - 0s 58ms/step - loss: 0.4895 - accuracy: 0.8958 - val_loss: 0.5481 - val_accuracy: 0.9167\n","Epoch 47/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4827 - accuracy: 0.9271 - val_loss: 0.5431 - val_accuracy: 0.9167\n","Epoch 48/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.4765 - accuracy: 0.9167 - val_loss: 0.5375 - val_accuracy: 0.9167\n","Epoch 49/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4732 - accuracy: 0.9167 - val_loss: 0.5307 - val_accuracy: 0.9167\n","Epoch 50/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.4648 - accuracy: 0.9271 - val_loss: 0.5267 - val_accuracy: 0.9167\n","Epoch 51/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.4598 - accuracy: 0.9167 - val_loss: 0.5228 - val_accuracy: 0.9167\n","Epoch 52/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4538 - accuracy: 0.9167 - val_loss: 0.5182 - val_accuracy: 0.9167\n","Epoch 53/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4511 - accuracy: 0.8958 - val_loss: 0.5142 - val_accuracy: 0.9167\n","Epoch 54/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.4436 - accuracy: 0.9271 - val_loss: 0.5105 - val_accuracy: 0.9167\n","Epoch 55/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.4412 - accuracy: 0.9271 - val_loss: 0.5056 - val_accuracy: 0.9167\n","Epoch 56/10000\n","3/3 [==============================] - 0s 27ms/step - loss: 0.4339 - accuracy: 0.9479 - val_loss: 0.5006 - val_accuracy: 0.9167\n","Epoch 57/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4287 - accuracy: 0.9375 - val_loss: 0.4981 - val_accuracy: 0.9167\n","Epoch 58/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.4245 - accuracy: 0.9479 - val_loss: 0.4940 - val_accuracy: 0.9167\n","Epoch 59/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4212 - accuracy: 0.9479 - val_loss: 0.4917 - val_accuracy: 0.9167\n","Epoch 60/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4153 - accuracy: 0.9583 - val_loss: 0.4831 - val_accuracy: 0.9167\n","Epoch 61/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.4130 - accuracy: 0.9583 - val_loss: 0.4773 - val_accuracy: 0.9167\n","Epoch 62/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4061 - accuracy: 0.9688 - val_loss: 0.4753 - val_accuracy: 0.9167\n","Epoch 63/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.4022 - accuracy: 0.9479 - val_loss: 0.4702 - val_accuracy: 0.9167\n","Epoch 64/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.4005 - accuracy: 0.9375 - val_loss: 0.4653 - val_accuracy: 0.9167\n","Epoch 65/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3939 - accuracy: 0.9479 - val_loss: 0.4628 - val_accuracy: 0.9167\n","Epoch 66/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3897 - accuracy: 0.9479 - val_loss: 0.4606 - val_accuracy: 0.9167\n","Epoch 67/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3860 - accuracy: 0.9583 - val_loss: 0.4609 - val_accuracy: 0.9583\n","Epoch 68/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3811 - accuracy: 0.9479 - val_loss: 0.4551 - val_accuracy: 0.9583\n","Epoch 69/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3775 - accuracy: 0.9479 - val_loss: 0.4465 - val_accuracy: 0.9167\n","Epoch 70/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3732 - accuracy: 0.9375 - val_loss: 0.4403 - val_accuracy: 0.9167\n","Epoch 71/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3709 - accuracy: 0.9375 - val_loss: 0.4410 - val_accuracy: 0.9583\n","Epoch 72/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3679 - accuracy: 0.9479 - val_loss: 0.4347 - val_accuracy: 0.9167\n","Epoch 73/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.3629 - accuracy: 0.9375 - val_loss: 0.4375 - val_accuracy: 0.9583\n","Epoch 74/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3585 - accuracy: 0.9583 - val_loss: 0.4365 - val_accuracy: 0.9167\n","Epoch 75/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3545 - accuracy: 0.9479 - val_loss: 0.4330 - val_accuracy: 0.9167\n","Epoch 76/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3570 - accuracy: 0.9375 - val_loss: 0.4302 - val_accuracy: 0.9167\n","Epoch 77/10000\n","3/3 [==============================] - 0s 20ms/step - loss: 0.3488 - accuracy: 0.9479 - val_loss: 0.4229 - val_accuracy: 0.9583\n","Epoch 78/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3457 - accuracy: 0.9375 - val_loss: 0.4172 - val_accuracy: 0.9583\n","Epoch 79/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3426 - accuracy: 0.9479 - val_loss: 0.4125 - val_accuracy: 0.9583\n","Epoch 80/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3388 - accuracy: 0.9375 - val_loss: 0.4096 - val_accuracy: 0.9583\n","Epoch 81/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.3354 - accuracy: 0.9479 - val_loss: 0.4058 - val_accuracy: 0.9583\n","Epoch 82/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3324 - accuracy: 0.9375 - val_loss: 0.4013 - val_accuracy: 0.9583\n","Epoch 83/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3289 - accuracy: 0.9479 - val_loss: 0.4035 - val_accuracy: 0.9583\n","Epoch 84/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.3253 - accuracy: 0.9375 - val_loss: 0.4022 - val_accuracy: 0.9167\n","Epoch 85/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3234 - accuracy: 0.9583 - val_loss: 0.3968 - val_accuracy: 0.9583\n","Epoch 86/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3199 - accuracy: 0.9688 - val_loss: 0.3976 - val_accuracy: 0.9167\n","Epoch 87/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3152 - accuracy: 0.9583 - val_loss: 0.3871 - val_accuracy: 0.9583\n","Epoch 88/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3122 - accuracy: 0.9583 - val_loss: 0.3863 - val_accuracy: 0.9583\n","Epoch 89/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.3083 - accuracy: 0.9375 - val_loss: 0.3827 - val_accuracy: 0.9583\n","Epoch 90/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.3077 - accuracy: 0.9479 - val_loss: 0.3811 - val_accuracy: 0.9167\n","Epoch 91/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3020 - accuracy: 0.9479 - val_loss: 0.3742 - val_accuracy: 0.9583\n","Epoch 92/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2988 - accuracy: 0.9479 - val_loss: 0.3727 - val_accuracy: 0.9583\n","Epoch 93/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.3010 - accuracy: 0.9375 - val_loss: 0.3687 - val_accuracy: 0.9583\n","Epoch 94/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2953 - accuracy: 0.9375 - val_loss: 0.3680 - val_accuracy: 0.9167\n","Epoch 95/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2902 - accuracy: 0.9375 - val_loss: 0.3633 - val_accuracy: 0.9583\n","Epoch 96/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2895 - accuracy: 0.9375 - val_loss: 0.3571 - val_accuracy: 0.9583\n","Epoch 97/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2850 - accuracy: 0.9583 - val_loss: 0.3565 - val_accuracy: 0.9583\n","Epoch 98/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2822 - accuracy: 0.9375 - val_loss: 0.3537 - val_accuracy: 0.9583\n","Epoch 99/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2817 - accuracy: 0.9479 - val_loss: 0.3591 - val_accuracy: 0.9167\n","Epoch 100/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2794 - accuracy: 0.9583 - val_loss: 0.3484 - val_accuracy: 0.9583\n","Epoch 101/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2767 - accuracy: 0.9583 - val_loss: 0.3416 - val_accuracy: 0.9583\n","Epoch 102/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2732 - accuracy: 0.9375 - val_loss: 0.3396 - val_accuracy: 0.9583\n","Epoch 103/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2695 - accuracy: 0.9479 - val_loss: 0.3394 - val_accuracy: 0.9583\n","Epoch 104/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.2668 - accuracy: 0.9479 - val_loss: 0.3382 - val_accuracy: 0.9583\n","Epoch 105/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2642 - accuracy: 0.9479 - val_loss: 0.3351 - val_accuracy: 0.9583\n","Epoch 106/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2668 - accuracy: 0.9688 - val_loss: 0.3354 - val_accuracy: 0.9167\n","Epoch 107/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2608 - accuracy: 0.9479 - val_loss: 0.3332 - val_accuracy: 0.9167\n","Epoch 108/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2601 - accuracy: 0.9375 - val_loss: 0.3241 - val_accuracy: 0.9583\n","Epoch 109/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2579 - accuracy: 0.9583 - val_loss: 0.3282 - val_accuracy: 0.9167\n","Epoch 110/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2529 - accuracy: 0.9375 - val_loss: 0.3231 - val_accuracy: 0.9583\n","Epoch 111/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2511 - accuracy: 0.9375 - val_loss: 0.3208 - val_accuracy: 0.9583\n","Epoch 112/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2518 - accuracy: 0.9479 - val_loss: 0.3150 - val_accuracy: 0.9583\n","Epoch 113/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2506 - accuracy: 0.9583 - val_loss: 0.3136 - val_accuracy: 0.9583\n","Epoch 114/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2446 - accuracy: 0.9479 - val_loss: 0.3126 - val_accuracy: 0.9583\n","Epoch 115/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2432 - accuracy: 0.9479 - val_loss: 0.3116 - val_accuracy: 0.9583\n","Epoch 116/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2409 - accuracy: 0.9479 - val_loss: 0.3127 - val_accuracy: 0.9167\n","Epoch 117/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2411 - accuracy: 0.9583 - val_loss: 0.3056 - val_accuracy: 0.9583\n","Epoch 118/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2362 - accuracy: 0.9479 - val_loss: 0.3059 - val_accuracy: 0.9583\n","Epoch 119/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2381 - accuracy: 0.9583 - val_loss: 0.3044 - val_accuracy: 0.9583\n","Epoch 120/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2364 - accuracy: 0.9479 - val_loss: 0.3025 - val_accuracy: 0.9583\n","Epoch 121/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.2307 - accuracy: 0.9479 - val_loss: 0.2989 - val_accuracy: 0.9583\n","Epoch 122/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2287 - accuracy: 0.9479 - val_loss: 0.2996 - val_accuracy: 0.9167\n","Epoch 123/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2268 - accuracy: 0.9375 - val_loss: 0.2930 - val_accuracy: 0.9583\n","Epoch 124/10000\n","3/3 [==============================] - 0s 29ms/step - loss: 0.2290 - accuracy: 0.9583 - val_loss: 0.2946 - val_accuracy: 0.9583\n","Epoch 125/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2226 - accuracy: 0.9375 - val_loss: 0.2904 - val_accuracy: 0.9583\n","Epoch 126/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2239 - accuracy: 0.9583 - val_loss: 0.2887 - val_accuracy: 0.9583\n","Epoch 127/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2208 - accuracy: 0.9479 - val_loss: 0.2910 - val_accuracy: 0.9167\n","Epoch 128/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2206 - accuracy: 0.9375 - val_loss: 0.2921 - val_accuracy: 0.9167\n","Epoch 129/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2182 - accuracy: 0.9375 - val_loss: 0.2846 - val_accuracy: 0.9583\n","Epoch 130/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2145 - accuracy: 0.9479 - val_loss: 0.2823 - val_accuracy: 0.9583\n","Epoch 131/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2147 - accuracy: 0.9479 - val_loss: 0.2826 - val_accuracy: 0.9167\n","Epoch 132/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2136 - accuracy: 0.9479 - val_loss: 0.2758 - val_accuracy: 0.9583\n","Epoch 133/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2118 - accuracy: 0.9375 - val_loss: 0.2715 - val_accuracy: 0.9583\n","Epoch 134/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.2080 - accuracy: 0.9479 - val_loss: 0.2711 - val_accuracy: 0.9583\n","Epoch 135/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2070 - accuracy: 0.9375 - val_loss: 0.2673 - val_accuracy: 0.9583\n","Epoch 136/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.2055 - accuracy: 0.9479 - val_loss: 0.2704 - val_accuracy: 0.9583\n","Epoch 137/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2025 - accuracy: 0.9479 - val_loss: 0.2673 - val_accuracy: 0.9583\n","Epoch 138/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.2012 - accuracy: 0.9479 - val_loss: 0.2642 - val_accuracy: 0.9583\n","Epoch 139/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.1996 - accuracy: 0.9479 - val_loss: 0.2608 - val_accuracy: 0.9583\n","Epoch 140/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.2031 - accuracy: 0.9479 - val_loss: 0.2612 - val_accuracy: 0.9583\n","Epoch 141/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1983 - accuracy: 0.9479 - val_loss: 0.2563 - val_accuracy: 0.9583\n","Epoch 142/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.1971 - accuracy: 0.9479 - val_loss: 0.2625 - val_accuracy: 0.9167\n","Epoch 143/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1946 - accuracy: 0.9583 - val_loss: 0.2583 - val_accuracy: 0.9583\n","Epoch 144/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1919 - accuracy: 0.9479 - val_loss: 0.2526 - val_accuracy: 0.9583\n","Epoch 145/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1903 - accuracy: 0.9479 - val_loss: 0.2522 - val_accuracy: 0.9583\n","Epoch 146/10000\n","3/3 [==============================] - 0s 20ms/step - loss: 0.1886 - accuracy: 0.9479 - val_loss: 0.2504 - val_accuracy: 0.9583\n","Epoch 147/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1892 - accuracy: 0.9583 - val_loss: 0.2547 - val_accuracy: 0.9167\n","Epoch 148/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1867 - accuracy: 0.9479 - val_loss: 0.2437 - val_accuracy: 0.9583\n","Epoch 149/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1838 - accuracy: 0.9479 - val_loss: 0.2482 - val_accuracy: 0.9583\n","Epoch 150/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1824 - accuracy: 0.9375 - val_loss: 0.2401 - val_accuracy: 0.9583\n","Epoch 151/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1815 - accuracy: 0.9479 - val_loss: 0.2439 - val_accuracy: 0.9583\n","Epoch 152/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1843 - accuracy: 0.9479 - val_loss: 0.2380 - val_accuracy: 0.9583\n","Epoch 153/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1820 - accuracy: 0.9479 - val_loss: 0.2416 - val_accuracy: 0.9583\n","Epoch 154/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1777 - accuracy: 0.9479 - val_loss: 0.2425 - val_accuracy: 0.9167\n","Epoch 155/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1787 - accuracy: 0.9375 - val_loss: 0.2388 - val_accuracy: 0.9583\n","Epoch 156/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1755 - accuracy: 0.9479 - val_loss: 0.2387 - val_accuracy: 0.9583\n","Epoch 157/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1737 - accuracy: 0.9479 - val_loss: 0.2304 - val_accuracy: 0.9583\n","Epoch 158/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1719 - accuracy: 0.9479 - val_loss: 0.2323 - val_accuracy: 0.9583\n","Epoch 159/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1740 - accuracy: 0.9479 - val_loss: 0.2334 - val_accuracy: 0.9583\n","Epoch 160/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1699 - accuracy: 0.9479 - val_loss: 0.2270 - val_accuracy: 0.9583\n","Epoch 161/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1714 - accuracy: 0.9583 - val_loss: 0.2279 - val_accuracy: 0.9583\n","Epoch 162/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1709 - accuracy: 0.9583 - val_loss: 0.2262 - val_accuracy: 0.9583\n","Epoch 163/10000\n","3/3 [==============================] - 0s 29ms/step - loss: 0.1668 - accuracy: 0.9479 - val_loss: 0.2245 - val_accuracy: 0.9583\n","Epoch 164/10000\n","3/3 [==============================] - 0s 26ms/step - loss: 0.1691 - accuracy: 0.9479 - val_loss: 0.2240 - val_accuracy: 0.9583\n","Epoch 165/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1640 - accuracy: 0.9479 - val_loss: 0.2203 - val_accuracy: 0.9583\n","Epoch 166/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1635 - accuracy: 0.9479 - val_loss: 0.2238 - val_accuracy: 0.9583\n","Epoch 167/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1624 - accuracy: 0.9479 - val_loss: 0.2165 - val_accuracy: 0.9583\n","Epoch 168/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1660 - accuracy: 0.9583 - val_loss: 0.2121 - val_accuracy: 0.9583\n","Epoch 169/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1597 - accuracy: 0.9479 - val_loss: 0.2154 - val_accuracy: 0.9583\n","Epoch 170/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1598 - accuracy: 0.9479 - val_loss: 0.2105 - val_accuracy: 0.9583\n","Epoch 171/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1577 - accuracy: 0.9479 - val_loss: 0.2154 - val_accuracy: 0.9583\n","Epoch 172/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1565 - accuracy: 0.9479 - val_loss: 0.2100 - val_accuracy: 0.9583\n","Epoch 173/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.1551 - accuracy: 0.9479 - val_loss: 0.2110 - val_accuracy: 0.9583\n","Epoch 174/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1539 - accuracy: 0.9479 - val_loss: 0.2099 - val_accuracy: 0.9583\n","Epoch 175/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1567 - accuracy: 0.9583 - val_loss: 0.2179 - val_accuracy: 0.9167\n","Epoch 176/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1530 - accuracy: 0.9479 - val_loss: 0.2111 - val_accuracy: 0.9583\n","Epoch 177/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1524 - accuracy: 0.9479 - val_loss: 0.2134 - val_accuracy: 0.9167\n","Epoch 178/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.1499 - accuracy: 0.9479 - val_loss: 0.2042 - val_accuracy: 0.9583\n","Epoch 179/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1492 - accuracy: 0.9479 - val_loss: 0.2102 - val_accuracy: 0.9583\n","Epoch 180/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1487 - accuracy: 0.9375 - val_loss: 0.1982 - val_accuracy: 0.9583\n","Epoch 181/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1499 - accuracy: 0.9375 - val_loss: 0.1948 - val_accuracy: 0.9583\n","Epoch 182/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1464 - accuracy: 0.9479 - val_loss: 0.1940 - val_accuracy: 0.9583\n","Epoch 183/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.1483 - accuracy: 0.9479 - val_loss: 0.1938 - val_accuracy: 0.9583\n","Epoch 184/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1463 - accuracy: 0.9479 - val_loss: 0.1911 - val_accuracy: 0.9583\n","Epoch 185/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1435 - accuracy: 0.9583 - val_loss: 0.1922 - val_accuracy: 0.9583\n","Epoch 186/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1439 - accuracy: 0.9479 - val_loss: 0.1985 - val_accuracy: 0.9583\n","Epoch 187/10000\n","3/3 [==============================] - 0s 16ms/step - loss: 0.1419 - accuracy: 0.9479 - val_loss: 0.1902 - val_accuracy: 0.9583\n","Epoch 188/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1451 - accuracy: 0.9375 - val_loss: 0.1888 - val_accuracy: 0.9583\n","Epoch 189/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1410 - accuracy: 0.9583 - val_loss: 0.1955 - val_accuracy: 0.9583\n","Epoch 190/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1468 - accuracy: 0.9583 - val_loss: 0.1892 - val_accuracy: 0.9583\n","Epoch 191/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1386 - accuracy: 0.9583 - val_loss: 0.1919 - val_accuracy: 0.9583\n","Epoch 192/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1380 - accuracy: 0.9479 - val_loss: 0.1921 - val_accuracy: 0.9583\n","Epoch 193/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1371 - accuracy: 0.9479 - val_loss: 0.1877 - val_accuracy: 0.9583\n","Epoch 194/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1367 - accuracy: 0.9479 - val_loss: 0.1876 - val_accuracy: 0.9583\n","Epoch 195/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.1358 - accuracy: 0.9479 - val_loss: 0.1824 - val_accuracy: 0.9583\n","Epoch 196/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1353 - accuracy: 0.9583 - val_loss: 0.1899 - val_accuracy: 0.9583\n","Epoch 197/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1348 - accuracy: 0.9479 - val_loss: 0.1856 - val_accuracy: 0.9583\n","Epoch 198/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1328 - accuracy: 0.9479 - val_loss: 0.1830 - val_accuracy: 0.9583\n","Epoch 199/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1373 - accuracy: 0.9479 - val_loss: 0.1830 - val_accuracy: 0.9583\n","Epoch 200/10000\n","3/3 [==============================] - 0s 27ms/step - loss: 0.1337 - accuracy: 0.9583 - val_loss: 0.1796 - val_accuracy: 0.9583\n","Epoch 201/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1318 - accuracy: 0.9479 - val_loss: 0.1748 - val_accuracy: 0.9583\n","Epoch 202/10000\n","3/3 [==============================] - 0s 32ms/step - loss: 0.1310 - accuracy: 0.9479 - val_loss: 0.1832 - val_accuracy: 0.9583\n","Epoch 203/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1311 - accuracy: 0.9583 - val_loss: 0.1858 - val_accuracy: 0.9583\n","Epoch 204/10000\n","3/3 [==============================] - 0s 30ms/step - loss: 0.1295 - accuracy: 0.9479 - val_loss: 0.1833 - val_accuracy: 0.9583\n","Epoch 205/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1286 - accuracy: 0.9479 - val_loss: 0.1811 - val_accuracy: 0.9583\n","Epoch 206/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1334 - accuracy: 0.9479 - val_loss: 0.1834 - val_accuracy: 0.9583\n","Epoch 207/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1290 - accuracy: 0.9375 - val_loss: 0.1739 - val_accuracy: 0.9583\n","Epoch 208/10000\n","3/3 [==============================] - 0s 36ms/step - loss: 0.1274 - accuracy: 0.9479 - val_loss: 0.1687 - val_accuracy: 0.9583\n","Epoch 209/10000\n","3/3 [==============================] - 0s 34ms/step - loss: 0.1261 - accuracy: 0.9479 - val_loss: 0.1695 - val_accuracy: 0.9583\n","Epoch 210/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1281 - accuracy: 0.9479 - val_loss: 0.1759 - val_accuracy: 0.9583\n","Epoch 211/10000\n","3/3 [==============================] - 0s 37ms/step - loss: 0.1242 - accuracy: 0.9479 - val_loss: 0.1709 - val_accuracy: 0.9583\n","Epoch 212/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1258 - accuracy: 0.9479 - val_loss: 0.1741 - val_accuracy: 0.9583\n","Epoch 213/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1273 - accuracy: 0.9375 - val_loss: 0.1638 - val_accuracy: 0.9583\n","Epoch 214/10000\n","3/3 [==============================] - 0s 27ms/step - loss: 0.1238 - accuracy: 0.9479 - val_loss: 0.1623 - val_accuracy: 0.9583\n","Epoch 215/10000\n","3/3 [==============================] - 0s 30ms/step - loss: 0.1274 - accuracy: 0.9583 - val_loss: 0.1711 - val_accuracy: 0.9583\n","Epoch 216/10000\n","3/3 [==============================] - 0s 26ms/step - loss: 0.1221 - accuracy: 0.9479 - val_loss: 0.1700 - val_accuracy: 0.9583\n","Epoch 217/10000\n","3/3 [==============================] - 0s 30ms/step - loss: 0.1238 - accuracy: 0.9583 - val_loss: 0.1709 - val_accuracy: 0.9583\n","Epoch 218/10000\n","3/3 [==============================] - 0s 30ms/step - loss: 0.1222 - accuracy: 0.9479 - val_loss: 0.1726 - val_accuracy: 0.9583\n","Epoch 219/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1204 - accuracy: 0.9479 - val_loss: 0.1649 - val_accuracy: 0.9583\n","Epoch 220/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1204 - accuracy: 0.9583 - val_loss: 0.1697 - val_accuracy: 0.9583\n","Epoch 221/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1218 - accuracy: 0.9479 - val_loss: 0.1627 - val_accuracy: 0.9583\n","Epoch 222/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1237 - accuracy: 0.9479 - val_loss: 0.1622 - val_accuracy: 0.9583\n","Epoch 223/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1188 - accuracy: 0.9479 - val_loss: 0.1585 - val_accuracy: 0.9583\n","Epoch 224/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.1186 - accuracy: 0.9583 - val_loss: 0.1655 - val_accuracy: 0.9583\n","Epoch 225/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.1202 - accuracy: 0.9479 - val_loss: 0.1571 - val_accuracy: 0.9583\n","Epoch 226/10000\n","3/3 [==============================] - 0s 24ms/step - loss: 0.1177 - accuracy: 0.9479 - val_loss: 0.1600 - val_accuracy: 0.9583\n","Epoch 227/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.1162 - accuracy: 0.9479 - val_loss: 0.1607 - val_accuracy: 0.9583\n","Epoch 228/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.1158 - accuracy: 0.9479 - val_loss: 0.1602 - val_accuracy: 0.9583\n","Epoch 229/10000\n","3/3 [==============================] - 0s 31ms/step - loss: 0.1152 - accuracy: 0.9479 - val_loss: 0.1576 - val_accuracy: 0.9583\n","Epoch 230/10000\n","3/3 [==============================] - 0s 37ms/step - loss: 0.1161 - accuracy: 0.9479 - val_loss: 0.1662 - val_accuracy: 0.9583\n","Epoch 231/10000\n","3/3 [==============================] - 0s 33ms/step - loss: 0.1153 - accuracy: 0.9479 - val_loss: 0.1522 - val_accuracy: 0.9583\n","Epoch 232/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.1180 - accuracy: 0.9583 - val_loss: 0.1480 - val_accuracy: 1.0000\n","Epoch 233/10000\n","3/3 [==============================] - 0s 26ms/step - loss: 0.1134 - accuracy: 0.9479 - val_loss: 0.1504 - val_accuracy: 0.9583\n","Epoch 234/10000\n","3/3 [==============================] - 0s 32ms/step - loss: 0.1142 - accuracy: 0.9583 - val_loss: 0.1593 - val_accuracy: 0.9583\n","Epoch 235/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1175 - accuracy: 0.9479 - val_loss: 0.1600 - val_accuracy: 0.9583\n","Epoch 236/10000\n","3/3 [==============================] - 0s 23ms/step - loss: 0.1120 - accuracy: 0.9479 - val_loss: 0.1546 - val_accuracy: 0.9583\n","Epoch 237/10000\n","3/3 [==============================] - 0s 22ms/step - loss: 0.1152 - accuracy: 0.9479 - val_loss: 0.1542 - val_accuracy: 0.9583\n","Epoch 238/10000\n","3/3 [==============================] - 0s 32ms/step - loss: 0.1120 - accuracy: 0.9479 - val_loss: 0.1491 - val_accuracy: 0.9583\n","Epoch 239/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1104 - accuracy: 0.9479 - val_loss: 0.1512 - val_accuracy: 0.9583\n","Epoch 240/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1158 - accuracy: 0.9479 - val_loss: 0.1448 - val_accuracy: 0.9583\n","Epoch 241/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1128 - accuracy: 0.9583 - val_loss: 0.1464 - val_accuracy: 0.9583\n","Epoch 242/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1103 - accuracy: 0.9583 - val_loss: 0.1468 - val_accuracy: 0.9583\n","Epoch 243/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1092 - accuracy: 0.9583 - val_loss: 0.1478 - val_accuracy: 0.9583\n","Epoch 244/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1088 - accuracy: 0.9479 - val_loss: 0.1493 - val_accuracy: 0.9583\n","Epoch 245/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1089 - accuracy: 0.9479 - val_loss: 0.1431 - val_accuracy: 0.9583\n","Epoch 246/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1124 - accuracy: 0.9583 - val_loss: 0.1491 - val_accuracy: 0.9583\n","Epoch 247/10000\n","3/3 [==============================] - 0s 21ms/step - loss: 0.1110 - accuracy: 0.9583 - val_loss: 0.1464 - val_accuracy: 0.9583\n","Epoch 248/10000\n","3/3 [==============================] - 0s 20ms/step - loss: 0.1088 - accuracy: 0.9479 - val_loss: 0.1407 - val_accuracy: 1.0000\n","Epoch 249/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1078 - accuracy: 0.9583 - val_loss: 0.1428 - val_accuracy: 0.9583\n","Epoch 250/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1067 - accuracy: 0.9479 - val_loss: 0.1447 - val_accuracy: 0.9583\n","Epoch 251/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1082 - accuracy: 0.9479 - val_loss: 0.1416 - val_accuracy: 0.9583\n","Epoch 252/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1082 - accuracy: 0.9479 - val_loss: 0.1464 - val_accuracy: 0.9583\n","Epoch 253/10000\n","3/3 [==============================] - 0s 26ms/step - loss: 0.1077 - accuracy: 0.9479 - val_loss: 0.1502 - val_accuracy: 0.9583\n","Epoch 254/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.1055 - accuracy: 0.9479 - val_loss: 0.1436 - val_accuracy: 0.9583\n","Epoch 255/10000\n","3/3 [==============================] - 0s 17ms/step - loss: 0.1102 - accuracy: 0.9479 - val_loss: 0.1451 - val_accuracy: 0.9583\n","Epoch 256/10000\n","3/3 [==============================] - 0s 26ms/step - loss: 0.1056 - accuracy: 0.9479 - val_loss: 0.1461 - val_accuracy: 0.9583\n","Epoch 257/10000\n","3/3 [==============================] - 0s 18ms/step - loss: 0.1042 - accuracy: 0.9479 - val_loss: 0.1424 - val_accuracy: 0.9583\n","Epoch 258/10000\n","3/3 [==============================] - 0s 19ms/step - loss: 0.1040 - accuracy: 0.9479 - val_loss: 0.1408 - val_accuracy: 0.9583\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f04600a5fd0>"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["## 테스트 데이터로 모델 평가하기"],"metadata":{"id":"Ohl_U7ohPVzO"}},{"cell_type":"code","source":["# TODO: 테스트 데이터와 정답으로 모델을 평가합니다.\n","# Hint: https://keras.io/api/models/model_training_apis/#evaluate-method\n","\n","test_loss, test_acc = model.evaluate(test_X, test_y)\n","print(f'Test Accuracy:{test_acc * 100:.2f}%')"],"metadata":{"id":"XR6xBHGVPU8U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679668339031,"user_tz":-540,"elapsed":10,"user":{"displayName":"민동준","userId":"06420174620131971170"}},"outputId":"e60f2e75-dede-4f33-a130-478d91789c18"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 28ms/step - loss: 0.1108 - accuracy: 0.9667\n","Test Accuracy:96.67%\n"]}]},{"cell_type":"markdown","source":["## 새로운 모델 만들어보기\n","테스트 데이터에서 96% 미만의 정확도가 나온 경우 모델 만들기부터 모든 것을 자유롭게 다시 수행하세요."],"metadata":{"id":"u4pa7rpzPoT5"}}]}